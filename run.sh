#python main.py --task_name TREC --do_train --do_eval --do_lower_case --data_dir ./trec_data/ --bert_model /root/workspace/pretrained_models/uncased_L-24_H-1024_A-16/  --max_seq_length 128 --train_batch_size 4 --learning_rate 3e-4 --num_train_epochs 1.0 --output_dir ./trec_large_output
#python main.py --task_name TREC --do_train --do_eval --do_lower_case --data_dir ./trec_data/ --bert_model /root/workspace/pretrained_models/uncased_L-12-H-768_A-12/  --max_seq_length 128 --train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 1.0 --output_dir ./test_output4
#python main.py --task_name TREC --do_train --do_eval --do_lower_case --data_dir ./wikiqa_data/ --bert_model /root/workspace/pretrained_models/uncased_L-12-H-768_A-12/  --max_seq_length 128 --train_batch_size 16 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir ./wiki_base_output3
#python main.py --task_name MRPC --do_train --do_eval --do_lower_case --data_dir ./MRPC/ --bert_model /root/workspace/pretrained_models/uncased_L-12-H-768_A-12/  --max_seq_length 128 --train_batch_size 16 --learning_rate 9e-6 --num_train_epochs 4.0 --output_dir ./mrpc_base_output1
#python main.py --task_name QQP --do_train --do_eval --do_lower_case --data_dir ./QQP/ --bert_model /root/workspace/pretrained_models/uncased_L-12-H-768_A-12/  --max_seq_length 128 --train_batch_size 16 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir ./qqp_base_output
#python main.py --task_name LCQMC --do_train --do_eval --do_lower_case --data_dir /root/workspace/qa_data/task_data/lcqmc --bert_model /root/workspace/pretrained_models/chinese_L-12_H-768_A-12  --max_seq_length 128 --train_batch_size 16 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir ./lrqmc_base_output
#python run_classifier.py --task_name MRPC --do_train --do_eval --do_lower_case --data_dir /root/workspace/qa_data/task_data/lcqmc --bert_model /root/workspace/pretrained_models/chinese_L-12_H-768_A-12  --max_seq_length 128 --train_batch_size 16 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir ./lrqmc_bert_output
#python main.py --task_name NLPCC --do_train --do_eval --do_lower_case --data_dir /root/workspace/qa_data/task_data/nlpcc2016/nlpcc-dbqa --bert_model /root/workspace/pretrained_models/chinese_L-12_H-768_A-12  --max_seq_length 128 --train_batch_size 16 --learning_rate 7e-6 --num_train_epochs 2.0 --output_dir ./nlpcc_output
python main.py --task_name CMEDQA --do_train --do_lower_case --data_dir /root/workspace/dataset/cMedQA2-master --bert_model /root/workspace/pretrained_models/chinese_L-12_H-768_A-12 --max_seq_length 128 --train_batch_size 4 --learning_rate 2e-5 --num_train_epochs 2.0 --output_dir ./fusion_cmed_output
