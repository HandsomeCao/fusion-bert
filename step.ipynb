{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bittorch4conda19cd795c40a4498c8bc6616258853fdd",
   "display_name": "Python 3.6.10 64-bit ('torch4': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "\n",
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<__main__.InputExample at 0x120241198>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import csv\n",
    "processor = MrpcProcessor()\n",
    "examples = processor.get_dev_examples('./mrpc_data/')\n",
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\""
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "examples[0].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['0', '1']"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "label_list = processor.get_labels()\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<__main__.InputFeatures at 0x10f4da2b0>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "features = convert_examples_to_features(examples, label_list, 256, tokenizer)\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[101,\n 2002,\n 2056,\n 1996,\n 9440,\n 2121,\n 7903,\n 2063,\n 11345,\n 2449,\n 2987,\n 1005,\n 1056,\n 4906,\n 1996,\n 2194,\n 1005,\n 1055,\n 2146,\n 1011,\n 2744,\n 3930,\n 5656,\n 1012,\n 102,\n 1000,\n 1996,\n 9440,\n 2121,\n 7903,\n 2063,\n 11345,\n 2449,\n 2515,\n 2025,\n 4906,\n 2256,\n 2146,\n 1011,\n 2744,\n 3930,\n 5656,\n 1012,\n 102,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "features[0].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel\n",
    "import torch.nn as nn\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[  101,  2002,  2056,  1996,  9440,  2121,  7903,  2063, 11345,  2449,\n          2987,  1005,  1056,  4906,  1996,  2194,  1005,  1055,  2146,  1011,\n          2744,  3930,  5656,  1012,   102,  1000,  1996,  9440,  2121,  7903,\n          2063, 11345,  2449,  2515,  2025,  4906,  2256,  2146,  1011,  2744,\n          3930,  5656,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "import torch\n",
    "input_id = torch.tensor([features[0].input_ids], dtype=torch.long)\n",
    "segment_ids = torch.tensor([features[0].segment_ids], dtype=torch.long)\n",
    "input_mask = torch.tensor([features[0].input_mask], dtype=torch.long)\n",
    "\n",
    "input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert(input_id, segment_ids, input_mask, output_all_encoded_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 256, 768])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "outputs[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 256, 768])"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sequence_output = outputs[0]\n",
    "sequence_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask_expanded = input_mask.unsqueeze(-1).expand(sequence_output.size()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "input_mask_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-8.8142e-01,  1.8816e-01, -4.3147e-01,  ..., -7.9133e-01,\n           9.3272e-01,  4.5988e-02],\n         [ 3.5908e-01, -1.1404e-01, -8.3745e-01,  ...,  3.6647e-01,\n           5.5146e-01, -5.4043e-01],\n         [-2.1388e-01, -1.3653e-01, -2.3459e-01,  ...,  2.8952e-01,\n          -1.3266e-01, -2.1964e-02],\n         ...,\n         [ 1.5847e-01,  1.4766e-01,  2.0529e-01,  ..., -4.3482e-03,\n           2.7301e-01, -7.3293e-02],\n         [-5.5554e-02, -2.3640e-01,  2.7843e-01,  ...,  4.0018e-01,\n           2.9027e-01, -1.1342e-01],\n         [-1.8808e-01, -3.5333e-01,  7.2856e-02,  ...,  3.7532e-01,\n           2.2211e-01, -3.4611e-01]]], grad_fn=<ThAddBackward>)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.8814,  0.1882, -0.4315,  ..., -0.7913,  0.9327,  0.0460],\n         [ 0.3591, -0.1140, -0.8374,  ...,  0.3665,  0.5515, -0.5404],\n         [-0.2139, -0.1365, -0.2346,  ...,  0.2895, -0.1327, -0.0220],\n         ...,\n         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]]],\n       grad_fn=<ThMulBackward>)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "sequence_output * input_mask_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ -11.0636,   -1.2235,    9.7954,    4.0469,   16.4276,    2.8415,\n            8.0583,   30.3949,    5.9854,   -0.6195,    8.9612,  -15.4292,\n            1.9297,   14.9294,  -10.7767,   11.7282,    8.9775,    7.8893,\n            2.0201,    9.1194,   -8.1628,  -12.7329,    6.7113,   19.1018,\n           19.1903,   -1.1186,    0.1073,    4.8895,   -5.6436,   -7.7154,\n           12.7934,   18.9433,  -25.5996,  -20.2311,   15.5622,   -1.7229,\n          -14.8629,   -6.9054,  -28.3170,   -6.0899,  -30.1646,   -7.5330,\n          -10.3559,    9.4625,   -6.4273,  -15.9965,   -4.8906,    1.1210,\n           -3.6906,   -0.6172,   -4.7344,    5.3236,   -3.8340,  -16.4559,\n           -3.4027,   32.8624,   -3.6920,  -16.6033,  -18.7660,  -11.5474,\n            5.8602,    1.1615,   -0.6110,  -27.9097,   -1.7852,   -1.7783,\n           -8.5259,   29.5530,  -46.9679,   -4.1414,  -20.3478,  -16.2187,\n            5.5075,    4.7283,  -11.6058,    5.7296,  -10.4476,    9.2362,\n            2.1492,   -7.9900,  -14.7464,   22.9806,   -5.3666,   36.8492,\n           -5.5957,    4.9588,    0.7350,   -2.4919,   -8.6160,   38.8583,\n           -4.9280,    0.3640,   18.7538,    9.6797,   -6.0741,    3.1558,\n           12.3821,    0.2753,  -17.5694,   28.5474,   15.2060,   -5.3636,\n            2.6303,    9.4750,   -6.3047,  -13.2398,    9.2726,   -5.5189,\n           -7.0629,   12.4444,    4.7134,   -1.6671,    6.6046,    3.3139,\n          -14.6273,    9.2571,  -12.1739,  -14.2240,   -8.7672,  -12.0275,\n            6.0659,    2.9580,   -4.3762,   28.5605,  -12.2757,    5.9387,\n          -19.0057,   11.4234,    1.8160,   -8.9746,    7.2628,   12.0303,\n            4.5771,  -24.6697,   -8.8242,   11.4679,    4.0309,   -8.9147,\n          -13.5569,   -1.3931,    5.3960,  -17.1851,   15.3586,   -6.6496,\n           20.3919,   -0.3261,   17.0694,   -3.7256,   -6.1887,    1.4965,\n            6.7789,    5.5106,    1.8319,   -3.9891,   -8.9865,   -7.7917,\n          -15.9866,   20.3616,    9.8438,    9.1991,    0.6934,   -7.4181,\n            5.9692,   11.6539,  -10.5597,   10.8629,    0.7846,    9.5108,\n          -16.0972,    6.4095,    1.0945,   -8.9505,   25.9971,   -4.7390,\n           15.3324,    7.3969,    3.4056,    6.5251,    0.5513,    6.3714,\n          -47.5231,   16.8288,   16.1616,    4.5903,   14.3746,  -17.3497,\n           11.7146,  -13.6097,    6.1384,   -5.9719,  -20.7518,  -20.6159,\n          -12.5956,  -15.9572,   -0.2043,  -18.8131,   -5.1610,    5.4936,\n          -15.9675,  -11.6498,   -6.9379,   -0.8553,   -3.5326,    4.5074,\n           -0.2973,  -17.3961,    5.4574,  -17.6417,    2.4165,   11.7431,\n           -9.9477,   14.2965,    8.6266,   -3.7249,    4.6276,  -14.7728,\n            2.3634,   -0.2064,  -15.1561,   -1.7650,   10.0547,    0.2221,\n          -21.1957,   16.9565,   -1.7243,   22.0753,   -0.7494,  -17.7862,\n           -9.6286,   34.5904,   -1.8026,  -12.9244,   16.0052,  -21.6733,\n          -12.0321,    2.0600,  -34.5509,   -8.6838,  -11.5188,  -20.3385,\n           -7.2716,    6.4346,    2.7476,    3.0074,    3.0404,  -18.3731,\n            1.2907,    1.4728,   -6.6386,  -16.9413,  -16.1770,  -27.5652,\n           10.9461,  -14.8593,   -3.8174,  -13.9151,    7.0535,   10.3031,\n            5.9689,   -8.9617,   17.0582,    9.8963,    7.2025,   15.2719,\n          -15.7075,  -29.0478,    3.8501,    3.0353,   -4.3594,    5.2188,\n            8.3057,  -15.2149,   -8.5296,   35.6107,   -2.9751,  -16.3629,\n            3.9855,   11.8026,   -2.6240,   -0.7231,   15.0332,   34.3376,\n          -17.1546,    1.9955,  -10.7772,   -9.2623,   10.2684,   12.9185,\n          -14.1491,   -5.3625,    2.0341,    1.2385,  -22.6148,  -12.5996,\n            9.2919,   12.4244,    4.3261,    2.2710,   14.3766,    4.1983,\n           -3.0168,  -13.4475,   20.1489,   10.3587,  -31.2025,   -0.5573,\n            8.6421,  -13.1297, -113.4703,   -7.1028,  -10.8111,  -11.9012,\n            8.1992,   -7.1443,  -16.2018,   -8.8533,  -21.3746,    6.1632,\n           -7.7261,  -16.9875,   19.6746,    5.0285,    9.9910,    2.6134,\n           10.5283,  -10.9935,    0.6363,   16.2900,    1.7990,  -27.8004,\n           -8.8889,  -11.0567,   21.9402,   24.5764,   -8.7538,    4.5508,\n          -15.6064,   -3.9901,   10.5325,  -15.1169,    7.8679,   -7.6630,\n           -5.9785,   -4.1615,   -4.9923,   -2.4829,  -10.1216,   -7.3022,\n           -2.8254,   -5.3936,   -5.3203,    0.5280,   32.5055,  -10.6045,\n           -7.6825,   -4.5861,   -2.2747,   15.0135,   14.5150,    0.4711,\n          -26.5402,   -1.3000,  -10.1584,    0.5909,   28.2334,    5.0218,\n          -12.1727,   -7.1998,    0.3498,   -6.1576,  -22.2276,    5.9841,\n           -7.5450,  -27.6329,  -23.8001,   -2.6967,   -7.2382,   -3.5078,\n            8.7862,    1.3504,  -20.0059,  -30.1063,    4.6869,  -13.8664,\n           11.6456,    3.1575,    6.8491,   -2.2446,   -6.0377,  -21.1744,\n           -5.7510,   -6.5324,   -1.5315,  -19.7716,    2.5163,    4.7458,\n          -15.7235,  -11.4062,    7.8693,    7.8920,   15.4363,    3.4363,\n           -6.9519,  -12.3462,   10.1775,   12.5132,  -10.5511,  -16.1912,\n           -3.1017,   -4.1721,   21.7349,    0.2057,   21.1428,    8.8553,\n          -10.1328,   -4.9819,    8.8507,    3.7496,   20.9550,  -16.1357,\n           21.2593,   -0.1806,  -17.2205,    0.9788,    1.0333,   31.2327,\n            0.4595,    8.4502,   10.9951,    6.8084,    7.1292,  -10.8114,\n          -18.0781,   -7.3117,   -7.8641,    1.4165,   -1.5170,   -2.9692,\n           -5.6774,   -6.2155,   -6.6397,   -2.0291,   10.1052,   -1.6230,\n            1.5779,  -15.7493,  -10.1119,    4.2028,    9.9146,   23.1546,\n            1.5014,   -4.5090,  -24.1416,   21.0667,   18.5962,    2.5217,\n          -12.0337,   15.4290,  -38.2911,    0.5650,   -7.7636,   -4.6560,\n            0.2851,   -5.6695,   16.8561,    9.7562,   -6.5716,  -33.0901,\n            4.4305,    7.2769,    1.1142,   -2.2666,   -4.7039,   19.1188,\n            5.5006,   -9.9918,  -22.5191,    5.5060,   -7.7088,  -12.0676,\n          -11.6857,  -17.8769,   -3.3926,    5.9983,   15.9072,  -18.0123,\n          -15.0159,    6.4129,   -4.1627,   -9.5094,  -21.6863,   -6.2220,\n           11.7527,    3.1515,    6.3996,  -10.4696,    6.4714,   -4.4811,\n           -5.4773,   14.2635,    0.3241,    0.8492,  -17.8386,   -5.7669,\n           19.5975,  -11.1302,    4.3957,    3.1797,   18.5575,    0.7631,\n           -1.7435,    5.4073,   15.6339,  -14.8939,   -0.0925,    9.5425,\n           -2.9663,    9.4323,  -20.8951,  -10.3796,   -9.4513,    3.7534,\n           11.3433,    1.9817,    0.4412,   -1.1171,   -4.3325,   15.7776,\n          -15.2678,  -13.1201,    1.9450,    8.6935,  -19.3763,    7.1780,\n            1.4509,    6.4253,   -9.7287,  -10.4215,   -1.1877,  -13.7285,\n            8.8838,   21.2047,    2.7618,   25.1814,  -16.3560,  -13.4173,\n           14.4297,   10.4914,  -14.8196,   -5.3496,    1.6747,   -5.9585,\n            3.5931,   -0.3551,   10.3782,   16.3263,  -17.8376,   28.4567,\n          -16.1293,    4.7153,   -6.9473,  -12.4708,    6.5266,  -16.1678,\n           -9.4508,   -3.6013,  -32.4701,   -2.9945,  -15.1411,   -5.2779,\n            9.0966,    8.9504,   31.2275,   18.8860,    6.3841,   10.1642,\n            1.5423,  -16.1016,    9.4905,   -2.8612,    8.0898,  -14.4944,\n           -5.9184,    6.5825,    4.1395,  -40.8119,   -8.0368,  -18.0238,\n          -16.8460,  -14.5074,    1.5874,   -4.4376,   14.0691,  -17.9218,\n           -8.9267,   10.2471,  -16.2809,    0.9534,    6.3863,   -7.2393,\n            4.5219,    3.2914,   -6.8985,    4.2793,   17.5945,   -2.3394,\n           -6.1643,   16.0947,   -4.1377,   25.9894,   18.2931,    9.1948,\n           -1.6654,    9.9733,    1.4393,  -15.1269,   11.7307,    4.0038,\n            0.2647,  -10.6725,   30.8064,   23.8262,  -11.3326,    3.1508,\n           -7.9156,  -18.5167,    1.7105,    9.8462,    5.0922,   -6.6833,\n           19.2728,    0.4544,    3.9670,   22.2774,  -26.5348,   -2.1416,\n           -3.4699,   15.3735,   -9.0881,   14.7184,  -12.9123,   20.0572,\n           -9.3818,   -9.2493,   -6.4773,    2.3434,    4.5103,    7.2664,\n           13.4733,   -0.0157,   14.8859,   14.0523,    7.2703,    8.5333,\n           16.3510,   10.5213,   22.4325,   19.9571,   -0.4398,   -1.9129,\n           -7.6580,    3.3566,    7.0862,    5.7835,    9.7360,   25.8515,\n           -3.1015,   36.2063,    3.9261,    6.0434,   14.1255,  -31.4384,\n            1.6987,    3.9044,    3.7840,  -14.7426,   -6.1111,   18.6487,\n           -4.9278,   17.4260,   -3.0194,   -0.3505,    0.4478,   26.7143,\n            1.0864,   -0.8397,  -10.0612,   14.7817,   -7.8050,    4.3246,\n          -13.6430,    4.1964,  -10.0919,    3.3437,    4.1768,    1.6076,\n           -3.2304,   -1.7364,   -3.3965,   -2.9708,   19.8764,  -11.2398,\n           10.2186,   -2.6461,   12.1999,    8.6605,   26.4420,    6.0755,\n           14.1851,   -5.9616,    4.4309,   10.8608,    0.2179,   -2.8552,\n           -9.2009,    7.3213,  -16.2512,  -16.9875,    5.7102,   11.6618,\n          -34.4078,   -5.0052,   -2.5197,   -9.0935,   -6.9805,    2.9300,\n           -5.4231,   -4.2512,   -9.1826,  -15.3459,   -2.0557,    5.0269,\n          -12.4090,    7.1547,    7.5464,   15.1205,    4.0242,   -2.5780,\n           -6.7988,    3.9167,   17.1963,   -3.3261,   -6.6221,  -17.9207,\n          -14.6741,    6.8621,  -16.3267,    2.1631,   18.2758,    6.8177,\n            9.3625,   -3.5085,   29.4047,   -3.5008,   -1.4447,  -15.7543,\n            3.0343,   -5.6006,   -2.2256,   -5.5369,   15.7723,   -3.4205,\n            8.4062,   -2.6147,    2.6842,  -11.9858,    4.3932,  -14.3352]],\n       grad_fn=<SumBackward1>)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sum_embedding = torch.sum(sequence_output* input_mask_expanded, 1)\n",
    "sum_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.,\n         44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44., 44.]])"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "sum_mask = input_mask_expanded.sum(1)\n",
    "sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "sum_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "mean_out = sum_embedding / sum_mask\n",
    "mean_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "mean_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 1536])"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "y = torch.cat((mean_out, mean_out), 1)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "04/01/2020 15:05:25 - INFO - util -   LOOKING AT ./trec_data/train.tsv\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5914"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from util import TrecProcessor\n",
    "processor = TrecProcessor()\n",
    "examples = processor.get_train_examples(data_dir='./trec_data/')\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'the IRON LADY ; A Biography of Margaret Thatcher by Hugo Young -LRB- Farrar , Straus & Giroux -RRB-'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "examples[0].text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}